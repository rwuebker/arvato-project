{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as IMBPipeline\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "from xgboost import plot_importance\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMEO_DEU_2015_MAP = {\n",
    "    '1A': 1,\n",
    "    '1B': 1,\n",
    "    '1C': 1,\n",
    "    '1D': 1,\n",
    "    '1E': 1,\n",
    "    '2A': 2,\n",
    "    '2B': 2,\n",
    "    '2C': 2,\n",
    "    '2D': 2,\n",
    "    '3A': 3,\n",
    "    '3B': 3,\n",
    "    '3C': 3,\n",
    "    '3D': 3,\n",
    "    '4A': 4,\n",
    "    '4B': 4,\n",
    "    '4C': 4,\n",
    "    '4D': 4,\n",
    "    '4E': 4,\n",
    "    '5A': 5,\n",
    "    '5B': 5,\n",
    "    '5C': 5,\n",
    "    '5D': 5,\n",
    "    '5E': 5,\n",
    "    '5F': 5,\n",
    "    '6A': 6,\n",
    "    '6B': 6,\n",
    "    '6C': 6,\n",
    "    '6D': 6,\n",
    "    '6E': 6,\n",
    "    '6F': 6,\n",
    "    '7A': 7,\n",
    "    '7B': 7,\n",
    "    '7C': 7,\n",
    "    '7D': 7,\n",
    "    '7E': 7,\n",
    "    '8A': 8,\n",
    "    '8B': 8,\n",
    "    '8C': 8,\n",
    "    '8D': 8,\n",
    "    '9A': 9,\n",
    "    '9B': 9,\n",
    "    '9C': 9,\n",
    "    '9D': 9,\n",
    "    '9E': 9\n",
    "}\n",
    "\n",
    "PRAEGENDE_JUGENDJAHRE_MAP = {\n",
    "    1: 0,\n",
    "    2: 1,\n",
    "    3: 0,\n",
    "    4: 1,\n",
    "    5: 0,\n",
    "    6: 1,\n",
    "    7: 1,\n",
    "    8: 0,\n",
    "    9: 1,\n",
    "    10: 0,\n",
    "    11: 1,\n",
    "    12: 0,\n",
    "    13: 1,\n",
    "    14: 0,\n",
    "    15: 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';', index_col='LNR', low_memory=False)\n",
    "test_df = pd.read_csv('data/Udacity_MAILOUT_052018_TEST.csv', sep=';', index_col='LNR', low_memory=False)\n",
    "metadata = pd.read_csv('data/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_order(val, mx, mn):\n",
    "    diff_from_low = val - mn \n",
    "    return mx - diff_from_low\n",
    "\n",
    "\n",
    "def default_clean(df, drop_threshold=20, testing=False):\n",
    "    df_ = df.copy()\n",
    "    \n",
    "    print('initial df shape: ', df_.shape)\n",
    "    keep_features = list(metadata[metadata['keep'] == 1]['feature_name'])\n",
    "    if 'RESPONSE' in df.columns:\n",
    "        keep_features.append('RESPONSE')\n",
    "    df_ = df_[keep_features]\n",
    "    \n",
    "    filter_ = df_['CAMEO_DEUG_2015'] != np.nan\n",
    "    df_.loc[filter_, 'CAMEO_DEUG_2015'] = pd.to_numeric(df_.loc[filter_, 'CAMEO_DEUG_2015'], errors='coerce')\n",
    "    \n",
    "    # set zero to negative one where zero means unknown\n",
    "    unknown_zero_features = list(metadata[metadata['unknown_zero'] == 1]['feature_name'])\n",
    "    for feature in unknown_zero_features:\n",
    "        df_.loc[df_[feature] == 0, feature] = -1\n",
    "        \n",
    "    # set nine to negative one where nine means unknown\n",
    "    unknown_nine_features = list(metadata[metadata['unknown_nine'] == 1]['feature_name'])\n",
    "    for feature in unknown_nine_features:\n",
    "        df_.loc[df_[feature] == 9, feature] = -1\n",
    "        \n",
    "        \n",
    "    # special cases\n",
    "    df_['CAMEO_DEUG_2015'].replace('X', np.nan, inplace=True)\n",
    "    #df_['OST_WEST_KZ'].replace('O', 1, inplace=True)\n",
    "    #df_['OST_WEST_KZ'].replace('W', 0, inplace=True)    \n",
    "    df_['CAMEO_DEU_2015'] = df_['CAMEO_DEU_2015'].apply(lambda x: x if x in CAMEO_DEU_2015_MAP else np.nan)\n",
    "    df_['PRAEGENDE_JUGENDJAHRE'] = df_['PRAEGENDE_JUGENDJAHRE'].apply(lambda x: PRAEGENDE_JUGENDJAHRE_MAP[x] if x in PRAEGENDE_JUGENDJAHRE_MAP else np.nan)\n",
    "    \n",
    "    # set -1 (unknown) to np.nan\n",
    "    df_ = df_.replace(-1, np.nan)\n",
    "    \n",
    "    \n",
    "    # change some numerical columns to categorical for one hot encoding:\n",
    "    cat_cols = list(metadata.loc[(metadata['type'] == 'categorical') & (metadata['keep'] == 1), 'feature_name'])\n",
    "    print('cat_cols: ', cat_cols)\n",
    "    for col in cat_cols:\n",
    "        if col in df_.columns:\n",
    "            df_[col] = np.where(df_[col].isnull(), df_[col], df_[col].astype('str'))\n",
    "\n",
    "    df_ = pd.get_dummies(df_, prefix=cat_cols, columns=cat_cols)\n",
    "    \n",
    "    \n",
    "    # reverse some cols so higher number = higher feature\n",
    "    reverse_cols = list(metadata.loc[metadata['needs_reverse']==1, 'feature_name'])\n",
    "    for col in reverse_cols:\n",
    "        if col in df_.columns:\n",
    "            series = df_[col]\n",
    "            df_[col] = df_[col].apply(reverse_order, args=(np.max(series), np.min(series)))\n",
    "            \n",
    "            \n",
    "    percent_missing = df_.isnull().sum() * 100 / len(df)\n",
    "    mv_df = pd.DataFrame({'column_name': df_.columns, 'percent_missing': percent_missing})\n",
    "    mv_cols = mv_df.loc[mv_df['percent_missing'] > drop_threshold]['column_name']    \n",
    "    df_ = df_.drop(list(mv_cols), axis=1)\n",
    "    \n",
    "    if False:\n",
    "        thresh = int(len(df_.columns) * 0.85)\n",
    "        grouped = df_.groupby(df_.RESPONSE)\n",
    "        pos = grouped.get_group(1)\n",
    "        neg = grouped.get_group(0)\n",
    "        neg = neg.dropna(thresh=thresh)\n",
    "        df_  = neg.append(pos, verify_integrity=True, ignore_index=False)\n",
    "        \n",
    "    #df_ = df_.loc[:, ~df_.columns.str.startswith('KB')]\n",
    "    \n",
    "    print('new df shape: ', df_.shape)\n",
    "    \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532\n",
      "initial df shape:  (42962, 366)\n",
      "cat_cols:  ['ANREDE_KZ', 'CAMEO_DEU_2015', 'D19_KONSUMTYP', 'GEBAEUDETYP', 'GEBAEUDETYP_RASTER', 'GFK_URLAUBERTYP', 'GREEN_AVANTGARDE', 'HEALTH_TYP', 'KBA05_HERSTTEMP', 'KBA05_MAXHERST', 'KBA05_MODTEMP', 'KBA05_SEG6', 'KONSUMNAEHE', 'NATIONALITAET_KZ', 'OST_WEST_KZ', 'PRAEGENDE_JUGENDJAHRE', 'TITEL_KZ', 'VERS_TYP', 'ZABEOTYP']\n",
      "new df shape:  (42962, 376)\n",
      "initial df shape:  (42833, 365)\n",
      "cat_cols:  ['ANREDE_KZ', 'CAMEO_DEU_2015', 'D19_KONSUMTYP', 'GEBAEUDETYP', 'GEBAEUDETYP_RASTER', 'GFK_URLAUBERTYP', 'GREEN_AVANTGARDE', 'HEALTH_TYP', 'KBA05_HERSTTEMP', 'KBA05_MAXHERST', 'KBA05_MODTEMP', 'KBA05_SEG6', 'KONSUMNAEHE', 'NATIONALITAET_KZ', 'OST_WEST_KZ', 'PRAEGENDE_JUGENDJAHRE', 'TITEL_KZ', 'VERS_TYP', 'ZABEOTYP']\n",
      "new df shape:  (42833, 375)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "532"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.sum(train_df['RESPONSE'] == 1))\n",
    "train_df_init = default_clean(train_df, drop_threshold=50)\n",
    "test_df_init = default_clean(test_df, drop_threshold=50, testing=True)\n",
    "np.sum(train_df_init['RESPONSE'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_sets():\n",
    "\n",
    "\n",
    "    inter_cols = list(set.intersection(set(train_df_init.columns), set(test_df_init.columns))) \n",
    "\n",
    "    X_train_full = train_df_init[inter_cols]\n",
    "\n",
    "    # \"Cardinality\" means the number of unique values in a column\n",
    "    # Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "    categorical_cols = [cname for cname in X_train_full.columns if\n",
    "                        X_train_full[cname].nunique() < 15 and\n",
    "                        X_train_full[cname].nunique() >= 2 and\n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "\n",
    "\n",
    "    # Select numerical columns\n",
    "    numerical_cols = [cname for cname in X_train_full.columns if \n",
    "                    X_train_full[cname].dtype in ['int64', 'float64', 'uint8']]\n",
    "    \n",
    "\n",
    "    # Keep selected columns only\n",
    "    my_cols = numerical_cols + categorical_cols\n",
    "\n",
    "    # supervised testing and full datasets\n",
    "    X_test = test_df_init[my_cols].copy()\n",
    "    X_total_train = train_df_init[my_cols].copy()\n",
    "    y_total_train = train_df_init['RESPONSE'].copy()\n",
    "\n",
    "    assert(list(X_total_train.columns) == list(X_test.columns))\n",
    "    \n",
    "    return X_total_train, y_total_train, X_test, numerical_cols, categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_total_train, y_total_train, X_test, numerical_cols, categorical_cols = get_initial_sets()\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "   # ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Benchmark Model\n",
    "model = LogisticRegression()\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('scaling', MinMaxScaler()),\n",
    "                              ('pca', PCA(n_components=150)),\n",
    "                              ('model', model)\n",
    "                     ])\n",
    "\n",
    "\n",
    "scores = cross_val_score(my_pipeline, X_total_train, y_total_train,\n",
    "                              cv=5,\n",
    "                              scoring='roc_auc')\n",
    "\n",
    "print('Mean AUC:', scores.mean())\n",
    "\n",
    "\n",
    "\n",
    "# has AUC of 0.6283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC: 0.7940265345555626\n",
      "AUC from out of sample:  0.562586826228467\n"
     ]
    }
   ],
   "source": [
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('scaling', MinMaxScaler()),\n",
    "                              ('pca', PCA(n_components=200))\n",
    "                     ])\n",
    "\n",
    "X = my_pipeline.fit_transform(X_total_train)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_total_train, test_size = 0.25, random_state = 42, stratify=y_total_train)\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_sample(X_train, y_train)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "scores = cross_val_score(model, X_smote, y_smote,\n",
    "                              cv=5,\n",
    "                              scoring='roc_auc')\n",
    "\n",
    "print('Mean AUC:', scores.mean())\n",
    "\n",
    "# out of sample test\n",
    "model = LogisticRegression()\n",
    "# train on smote data\n",
    "model.fit(X_smote, y_smote)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "print('AUC from out of sample: ', roc_auc_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=-1, num_parallel_tree=None,\n",
      "              random_state=None, reg_alpha=None, reg_lambda=None,\n",
      "              scale_pos_weight=180, subsample=None, tree_method=None,\n",
      "              validate_parameters=None, verbosity=None)\n",
      "Mean AUC: 0.9911072088046998\n",
      "AUC from out of sample:  0.5172373099037185\n",
      "BalancedRandomForestClassifier(n_jobs=-1)\n",
      "Mean AUC: 0.9930000637369828\n",
      "AUC from out of sample:  0.5163364434842764\n",
      "BalancedBaggingClassifier()\n",
      "Mean AUC: 0.9908648095963128\n",
      "AUC from out of sample:  0.5123413737964821\n",
      "HistGradientBoostingClassifier()\n",
      "Mean AUC: 0.9912463219780694\n",
      "AUC from out of sample:  0.5178202860091405\n"
     ]
    }
   ],
   "source": [
    "models = [xgb.XGBClassifier(scale_pos_weight=180, n_jobs=-1), BalancedRandomForestClassifier(n_jobs=-1), \n",
    "          BalancedBaggingClassifier(), HistGradientBoostingClassifier()]\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    print(model)\n",
    "    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor)\n",
    "                     ])\n",
    "\n",
    "    X = my_pipeline.fit_transform(X_total_train)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_total_train, test_size = 0.25, \n",
    "                                                        random_state = 42, stratify=y_total_train)\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_smote, y_smote = smote.fit_sample(X_train, y_train)\n",
    "\n",
    "    scores = cross_val_score(model, X_smote, y_smote,\n",
    "                              cv=5,\n",
    "                              scoring='roc_auc')\n",
    "\n",
    "    print('Mean AUC:', scores.mean())\n",
    "\n",
    "    # out of sample test\n",
    "    # train on smote data\n",
    "    model.fit(X_smote, y_smote)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    print('AUC from out of sample: ', roc_auc_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing SMOTE with keeping some imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    31822\n",
      "1    12728\n",
      "Name: RESPONSE, dtype: int64\n",
      "Mean AUC: 0.7913098047540492\n",
      "AUC from out of sample:  0.5266981792717086\n"
     ]
    }
   ],
   "source": [
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('scaling', MinMaxScaler()),\n",
    "                              ('pca', PCA(n_components=200))\n",
    "                     ])\n",
    "\n",
    "X = my_pipeline.fit_transform(X_total_train)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_total_train, test_size = 0.25, random_state = 42, stratify=y_total_train)\n",
    "\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.40)\n",
    "\n",
    "X_smote, y_smote = smote.fit_sample(X_train, y_train)\n",
    "print(y_smote.value_counts())\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "scores = cross_val_score(model, X_smote, y_smote,\n",
    "                              cv=5,\n",
    "                              scoring='roc_auc')\n",
    "\n",
    "print('Mean AUC:', scores.mean())\n",
    "\n",
    "# out of sample test\n",
    "model = LogisticRegression()\n",
    "# train on smote data\n",
    "model.fit(X_smote, y_smote)\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "print('AUC from out of sample: ', roc_auc_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, colsample_bylevel=None,\n",
      "              colsample_bynode=None, colsample_bytree=None, gamma=None,\n",
      "              gpu_id=None, importance_type='gain', interaction_constraints=None,\n",
      "              learning_rate=None, max_delta_step=None, max_depth=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              n_estimators=100, n_jobs=-1, num_parallel_tree=None,\n",
      "              random_state=None, reg_alpha=None, reg_lambda=None,\n",
      "              scale_pos_weight=180, subsample=None, tree_method=None,\n",
      "              validate_parameters=None, verbosity=None)\n",
      "0    31822\n",
      "1    12728\n",
      "Name: RESPONSE, dtype: int64\n",
      "Mean AUC: 0.9855540880772512\n",
      "AUC from out of sample:  0.5271128188117352\n",
      "BalancedRandomForestClassifier(n_jobs=-1)\n",
      "0    31822\n",
      "1    12728\n",
      "Name: RESPONSE, dtype: int64\n",
      "Mean AUC: 0.9899767918575177\n",
      "AUC from out of sample:  0.5215796845053811\n",
      "BalancedBaggingClassifier()\n",
      "0    31822\n",
      "1    12728\n",
      "Name: RESPONSE, dtype: int64\n",
      "Mean AUC: 0.9853500562310341\n",
      "AUC from out of sample:  0.5208255366924098\n",
      "HistGradientBoostingClassifier()\n",
      "0    31822\n",
      "1    12728\n",
      "Name: RESPONSE, dtype: int64\n",
      "Mean AUC: 0.9855827949953383\n",
      "AUC from out of sample:  0.5\n"
     ]
    }
   ],
   "source": [
    "models = [xgb.XGBClassifier(scale_pos_weight=180, n_jobs=-1), BalancedRandomForestClassifier(n_jobs=-1), \n",
    "          BalancedBaggingClassifier(), HistGradientBoostingClassifier()]\n",
    "\n",
    "for model in models:\n",
    "    \n",
    "    print(model)\n",
    "    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor)\n",
    "                     ])\n",
    "\n",
    "    X = my_pipeline.fit_transform(X_total_train)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_total_train, test_size = 0.25, \n",
    "                                                        random_state = 42, stratify=y_total_train)\n",
    "\n",
    "    smote = SMOTE(random_state=42, sampling_strategy=0.40)\n",
    "    X_smote, y_smote = smote.fit_sample(X_train, y_train)\n",
    "    print(y_smote.value_counts())\n",
    "\n",
    "    scores = cross_val_score(model, X_smote, y_smote,\n",
    "                              cv=5,\n",
    "                              scoring='roc_auc')\n",
    "\n",
    "    print('Mean AUC:', scores.mean())\n",
    "\n",
    "    # out of sample test\n",
    "    # train on smote data\n",
    "    model.fit(X_smote, y_smote)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    print('AUC from out of sample: ', roc_auc_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
